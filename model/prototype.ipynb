{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Row ID         Order ID  Order Date   Ship Date     Ship Mode Customer ID  \\\n",
      "0   32298   CA-2012-124891  31-07-2012  31-07-2012      Same Day    RH-19495   \n",
      "1   26341    IN-2013-77878  05-02-2013  07-02-2013  Second Class    JR-16210   \n",
      "2   25330    IN-2013-71249  17-10-2013  18-10-2013   First Class    CR-12730   \n",
      "3   13524  ES-2013-1579342  28-01-2013  30-01-2013   First Class    KM-16375   \n",
      "4   47221     SG-2013-4320  05-11-2013  06-11-2013      Same Day     RH-9495   \n",
      "\n",
      "      Customer Name      Segment           City            State  ...  \\\n",
      "0       Rick Hansen     Consumer  New York City         New York  ...   \n",
      "1     Justin Ritter    Corporate     Wollongong  New South Wales  ...   \n",
      "2      Craig Reiter     Consumer       Brisbane       Queensland  ...   \n",
      "3  Katherine Murray  Home Office         Berlin           Berlin  ...   \n",
      "4       Rick Hansen     Consumer          Dakar            Dakar  ...   \n",
      "\n",
      "         Product ID    Category Sub-Category  \\\n",
      "0   TEC-AC-10003033  Technology  Accessories   \n",
      "1   FUR-CH-10003950   Furniture       Chairs   \n",
      "2   TEC-PH-10004664  Technology       Phones   \n",
      "3   TEC-PH-10004583  Technology       Phones   \n",
      "4  TEC-SHA-10000501  Technology      Copiers   \n",
      "\n",
      "                                        Product Name     Sales Quantity  \\\n",
      "0  Plantronics CS510 - Over-the-Head monaural Wir...  2309.650        7   \n",
      "1          Novimex Executive Leather Armchair, Black  3709.395        9   \n",
      "2                  Nokia Smart Phone, with Caller ID  5175.171        9   \n",
      "3                     Motorola Smart Phone, Cordless  2892.510        5   \n",
      "4                     Sharp Wireless Fax, High-Speed  2832.960        8   \n",
      "\n",
      "  Discount    Profit  Shipping Cost  Order Priority  \n",
      "0      0.0  762.1845         933.57        Critical  \n",
      "1      0.1 -288.7650         923.63        Critical  \n",
      "2      0.1  919.9710         915.49          Medium  \n",
      "3      0.1  -96.5400         910.16          Medium  \n",
      "4      0.0  311.5200         903.04        Critical  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "DataFrame shape : (51290, 24)\n",
      "\n",
      "DataFrame  head:\n",
      "   Row ID         Order ID  Order Date   Ship Date     Ship Mode Customer ID  \\\n",
      "0   32298   CA-2012-124891  31-07-2012  31-07-2012      Same Day    RH-19495   \n",
      "1   26341    IN-2013-77878  05-02-2013  07-02-2013  Second Class    JR-16210   \n",
      "2   25330    IN-2013-71249  17-10-2013  18-10-2013   First Class    CR-12730   \n",
      "3   13524  ES-2013-1579342  28-01-2013  30-01-2013   First Class    KM-16375   \n",
      "4   47221     SG-2013-4320  05-11-2013  06-11-2013      Same Day     RH-9495   \n",
      "\n",
      "      Customer Name      Segment           City            State  ...  \\\n",
      "0       Rick Hansen     Consumer  New York City         New York  ...   \n",
      "1     Justin Ritter    Corporate     Wollongong  New South Wales  ...   \n",
      "2      Craig Reiter     Consumer       Brisbane       Queensland  ...   \n",
      "3  Katherine Murray  Home Office         Berlin           Berlin  ...   \n",
      "4       Rick Hansen     Consumer          Dakar            Dakar  ...   \n",
      "\n",
      "         Product ID    Category Sub-Category  \\\n",
      "0   TEC-AC-10003033  Technology  Accessories   \n",
      "1   FUR-CH-10003950   Furniture       Chairs   \n",
      "2   TEC-PH-10004664  Technology       Phones   \n",
      "3   TEC-PH-10004583  Technology       Phones   \n",
      "4  TEC-SHA-10000501  Technology      Copiers   \n",
      "\n",
      "                                        Product Name     Sales Quantity  \\\n",
      "0  Plantronics CS510 - Over-the-Head monaural Wir...  2309.650        7   \n",
      "1          Novimex Executive Leather Armchair, Black  3709.395        9   \n",
      "2                  Nokia Smart Phone, with Caller ID  5175.171        9   \n",
      "3                     Motorola Smart Phone, Cordless  2892.510        5   \n",
      "4                     Sharp Wireless Fax, High-Speed  2832.960        8   \n",
      "\n",
      "  Discount    Profit  Shipping Cost  Order Priority  \n",
      "0      0.0  762.1845         933.57        Critical  \n",
      "1      0.1 -288.7650         923.63        Critical  \n",
      "2      0.1  919.9710         915.49          Medium  \n",
      "3      0.1  -96.5400         910.16          Medium  \n",
      "4      0.0  311.5200         903.04        Critical  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "DataFrame columns: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'City', 'State', 'Country', 'Postal Code', 'Market', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping Cost', 'Order Priority']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/mohammednihal/Desktop/XAI/data/Global Superstore.csv',encoding='ISO-8859-1')\n",
    "df = df.drop([col  for col in df.columns if 'Unnamed' in col], axis=1)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"DataFrame shape :\" , df.shape)\n",
    "print(\"\\nDataFrame  head:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataFrame columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Data Overview:\n",
      "Initial shape: (51290, 24)\n",
      "Columns: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'City', 'State', 'Country', 'Postal Code', 'Market', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping Cost', 'Order Priority']\n",
      "\n",
      "Missing Values Overview:\n",
      "Row ID                0\n",
      "Order ID              0\n",
      "Order Date            0\n",
      "Ship Date             0\n",
      "Ship Mode             0\n",
      "Customer ID           0\n",
      "Customer Name         0\n",
      "Segment               0\n",
      "City                  0\n",
      "State                 0\n",
      "Country               0\n",
      "Postal Code       41296\n",
      "Market                0\n",
      "Region                0\n",
      "Product ID            0\n",
      "Category              0\n",
      "Sub-Category          0\n",
      "Product Name          0\n",
      "Sales                 0\n",
      "Quantity              0\n",
      "Discount              0\n",
      "Profit                0\n",
      "Shipping Cost         0\n",
      "Order Priority        0\n",
      "dtype: int64\n",
      "\n",
      "Cleaning Data...\n",
      "   Row ID         Order ID Order Date  Ship Date       Ship Mode Customer ID  \\\n",
      "0   48883     HU-2011-1220 2011-01-01 2011-01-05    Second Class      AT-735   \n",
      "1   22253    IN-2011-47883 2011-01-01 2011-01-08  Standard Class    JH-15985   \n",
      "2   11731  IT-2011-3647632 2011-01-01 2011-01-05    Second Class    EM-14140   \n",
      "3   22254    IN-2011-47883 2011-01-01 2011-01-08  Standard Class    JH-15985   \n",
      "4   22255    IN-2011-47883 2011-01-01 2011-01-08  Standard Class    JH-15985   \n",
      "\n",
      "   Customer Name      Segment         City            State  ...  \\\n",
      "0  annie thurman     Consumer     budapest         budapest  ...   \n",
      "1    joseph holt     Consumer  wagga wagga  new south wales  ...   \n",
      "2   eugene moren  Home Office    stockholm        stockholm  ...   \n",
      "3    joseph holt     Consumer  wagga wagga  new south wales  ...   \n",
      "4    joseph holt     Consumer  wagga wagga  new south wales  ...   \n",
      "\n",
      "          Category  Sub-Category                             Product Name  \\\n",
      "0  Office Supplies       Storage                  tenex box, single width   \n",
      "1  Office Supplies      Supplies                 acme trimmer, high speed   \n",
      "2  Office Supplies         Paper              enermax note cards, premium   \n",
      "3  Office Supplies         Paper  eaton computer printout paper, 8.5 x 11   \n",
      "4        Furniture   Furnishings               eldon light bulb, duo pack   \n",
      "\n",
      "     Sales Quantity Discount  Profit Shipping Cost  Order Priority  Unit_Price  \n",
      "0   66.120      4.0      0.0  29.640          8.17            high    7.077500  \n",
      "1  120.366      3.0      0.1  36.036          9.72          medium   27.633333  \n",
      "2   44.865      3.0      0.5 -26.055          4.82            high   44.066667  \n",
      "3   55.242      2.0      0.1  15.342          1.80          medium   21.166667  \n",
      "4  113.670      5.0      0.1  37.770          4.70          medium   15.822222  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Data cleaned. Remaining rows: 51244\n",
      "Columns Dropped Successfully\n",
      "Remaining columns: ['Order Date', 'Segment', 'Category', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping Cost', 'Order Priority', 'Unit_Price']\n",
      "\n",
      "Data Cleaning Pipeline Executed Successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def data_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Convert datetime columns\n",
    "    datetime_columns = ['Order Date', 'Ship Date']\n",
    "    for col in datetime_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], dayfirst=True, errors='coerce')\n",
    "\n",
    "    # Remove exact duplicates\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "    # Remove potential duplicates based on Customer ID, Sales, and Profit\n",
    "    df_clean = df_clean.drop_duplicates(subset=['Customer ID', 'Sales', 'Profit'], keep='first')\n",
    "\n",
    "    # Fill missing numerical values with median\n",
    "    numeric_columns = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in numeric_columns:\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "\n",
    "    # Fill missing categorical values with mode\n",
    "    categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        if not df_clean[col].mode().empty:\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0])\n",
    "\n",
    "    # Handle outliers using IQR\n",
    "    def handle_outliers(series):\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "    numeric_cols_for_outliers = ['Sales', 'Profit', 'Quantity', 'Discount', 'Shipping Cost']\n",
    "    for col in numeric_cols_for_outliers:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = handle_outliers(df_clean[col])\n",
    "\n",
    "    # Clip specific ranges\n",
    "    df_clean['Profit'] = df_clean['Profit'].clip(lower=-1500, upper=10000)\n",
    "    df_clean['Quantity'] = df_clean['Quantity'].clip(lower=1)\n",
    "    df_clean['Discount'] = df_clean['Discount'].clip(0, 1)\n",
    "    df_clean['Shipping Cost'] = df_clean['Shipping Cost'].clip(lower=0, upper=1000)\n",
    "\n",
    "    # Calculate Unit Price safely\n",
    "    discount_safe = (1 - df_clean['Discount']).replace(0, np.nan)\n",
    "    df_clean['Unit_Price'] = (df_clean['Sales'] - df_clean['Shipping Cost'] - df_clean['Profit']) / (df_clean['Quantity'] * discount_safe)\n",
    "\n",
    "    # Standardize categorical variables\n",
    "    standardize_cols = ['Customer Name', 'City', 'State', 'Product Name', 'Country', 'Order Priority', 'Region', 'Market']\n",
    "    for col in standardize_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].str.lower()\n",
    "\n",
    "    # Remove any remaining NaNs\n",
    "    df_clean = df_clean.dropna()\n",
    "\n",
    "    # Sort by Order Date\n",
    "    if 'Order Date' in df_clean.columns:\n",
    "        df_clean = df_clean.sort_values(by='Order Date', ascending=True)\n",
    "\n",
    "    df_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(df_clean.head())\n",
    "\n",
    "    # Optional: Save to CSV\n",
    "    df_clean.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def Initial_col_drop(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    columns_to_drop = [\n",
    "        'Row ID', 'Order ID', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name',\n",
    "        'City', 'State', 'Country', 'Postal Code', 'Market', 'Region', 'Product ID',\n",
    "        'Product Name', 'Year-Month', 'Sub-Category'\n",
    "    ]\n",
    "    return df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "\n",
    "def main(df: pd.DataFrame):\n",
    "    \n",
    "    try:\n",
    "        # Initial data overview\n",
    "        print(\"\\nInitial Data Overview:\")\n",
    "        print(f\"Initial shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(\"\\nMissing Values Overview:\")\n",
    "        print(df.isnull().sum())\n",
    "\n",
    "        # Cleaning data\n",
    "        print(\"\\nCleaning Data...\")\n",
    "        cleaned_df = data_cleaning(df)\n",
    "        print(f\"Data cleaned. Remaining rows: {len(cleaned_df)}\")\n",
    "\n",
    "        # Dropping unnecessary columns\n",
    "        cleaned_df = Initial_col_drop(cleaned_df)\n",
    "        print(\"Columns Dropped Successfully\")\n",
    "        print(f\"Remaining columns: {cleaned_df.columns.tolist()}\")\n",
    "\n",
    "        return cleaned_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Data Cleaning Pipeline: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try:\n",
    "        df = df  \n",
    "        cleaned_df = main(df)\n",
    "\n",
    "        if cleaned_df is not None:\n",
    "            print(\"\\nData Cleaning Pipeline Executed Successfully!\")\n",
    "        else:\n",
    "            print(\"\\nData Cleaning Pipeline Failed.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Data file not found. Please ensure 'your_data_file.csv' exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Office Supplies\n",
      "1    Office Supplies\n",
      "2    Office Supplies\n",
      "3    Office Supplies\n",
      "4          Furniture\n",
      "5    Office Supplies\n",
      "6         Technology\n",
      "7         Technology\n",
      "8          Furniture\n",
      "9    Office Supplies\n",
      "Name: Category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_df['Category'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2011-01-01\n",
      "1   2011-01-01\n",
      "2   2011-01-01\n",
      "3   2011-01-01\n",
      "4   2011-01-01\n",
      "Name: Order Date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "column_name = 'Order Date'\n",
    "print(cleaned_df[column_name].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Time-Based Features: 0\n",
      "Missing values in Lag Features: 7\n",
      "Missing values in Rolling Features: 93\n",
      "Missing values in Cumulative Features: 93\n",
      "Missing values in Category-Level Features: 93\n",
      "Missing values in Growth and Extra Features: 183\n",
      "Missing values in Interaction Features: 183\n",
      "Missing values in Missing Value Handling: 0\n",
      "   Day_of_Week_sin  Day_of_Week_cos  Month_sin  Month_cos  Is_Weekend  \\\n",
      "0        -0.974928        -0.222521        0.5   0.866025           1   \n",
      "1        -0.974928        -0.222521        0.5   0.866025           1   \n",
      "2        -0.974928        -0.222521        0.5   0.866025           1   \n",
      "3        -0.974928        -0.222521        0.5   0.866025           1   \n",
      "4        -0.974928        -0.222521        0.5   0.866025           1   \n",
      "5        -0.974928        -0.222521        0.5   0.866025           1   \n",
      "6        -0.781831         0.623490        0.5   0.866025           1   \n",
      "7         0.000000         1.000000        0.5   0.866025           0   \n",
      "8         0.000000         1.000000        0.5   0.866025           0   \n",
      "9         0.000000         1.000000        0.5   0.866025           0   \n",
      "\n",
      "   Sales_Lag_7  Rolling_Sales_7_Office Supplies  \\\n",
      "0   172.398334                       172.398334   \n",
      "1   172.398334                       172.398334   \n",
      "2   172.398334                       172.398334   \n",
      "3   172.398334                       172.398334   \n",
      "4   172.398334                       172.398334   \n",
      "5   172.398334                       172.398334   \n",
      "6   172.398334                       172.398334   \n",
      "7    66.120000                       172.398334   \n",
      "8   120.366000                       172.398334   \n",
      "9    44.865000                       172.398334   \n",
      "\n",
      "   Rolling_Profit_7_Office Supplies  Rolling_Sales_7_Furniture  \\\n",
      "0                         25.933241                 172.398334   \n",
      "1                         25.933241                 172.398334   \n",
      "2                         25.933241                 172.398334   \n",
      "3                         25.933241                 172.398334   \n",
      "4                         25.933241                 172.398334   \n",
      "5                         25.933241                 172.398334   \n",
      "6                         25.933241                 172.398334   \n",
      "7                         25.933241                 172.398334   \n",
      "8                         25.933241                 172.398334   \n",
      "9                         25.933241                 172.398334   \n",
      "\n",
      "   Rolling_Profit_7_Furniture  Rolling_Sales_7_Technology  \\\n",
      "0                    25.28992                  172.398334   \n",
      "1                    25.28992                  172.398334   \n",
      "2                    25.28992                  172.398334   \n",
      "3                    25.28992                  172.398334   \n",
      "4                    25.28992                  172.398334   \n",
      "5                    25.28992                  172.398334   \n",
      "6                    25.28992                  172.398334   \n",
      "7                    25.28992                  172.398334   \n",
      "8                    25.28992                  172.398334   \n",
      "9                    25.28992                  172.398334   \n",
      "\n",
      "   Rolling_Profit_7_Technology  Cumulative_Sales  Cumulative_Profit  \\\n",
      "0                      8.83517           66.1200          29.640000   \n",
      "1                      8.83517          186.4860          65.676000   \n",
      "2                      8.83517          231.3510          39.621000   \n",
      "3                      8.83517          286.5930          54.963000   \n",
      "4                      8.83517          113.6700          37.770000   \n",
      "5                      8.83517          694.8930         147.002688   \n",
      "6                      8.83517          314.2200           3.120000   \n",
      "7                      8.83517          383.5200           6.540000   \n",
      "8                      8.83517          324.8505         -17.453813   \n",
      "9                      8.83517          746.7330         154.772688   \n",
      "\n",
      "   Sales_vs_Category_Avg  Category_Sales_Growth  Rolling_30_Sales  \\\n",
      "0             -30.066184               0.000000        172.398334   \n",
      "1              24.179816               0.820417        172.398334   \n",
      "2             -51.321184              -0.627262        172.398334   \n",
      "3             -40.944184               0.231294        172.398334   \n",
      "4            -165.296440               0.000000        172.398334   \n",
      "5             312.113816               6.391115        172.398334   \n",
      "6              10.538986               0.000000        172.398334   \n",
      "7            -234.381014              -0.779454        172.398334   \n",
      "8             -67.785940               0.857838        172.398334   \n",
      "9             -44.346184              -0.873035        172.398334   \n",
      "\n",
      "   Weekend_Sales  \n",
      "0         66.120  \n",
      "1        120.366  \n",
      "2         44.865  \n",
      "3         55.242  \n",
      "4        113.670  \n",
      "5        408.300  \n",
      "6        314.220  \n",
      "7          0.000  \n",
      "8          0.000  \n",
      "9          0.000  \n",
      "['Day_of_Week_sin', 'Day_of_Week_cos', 'Month_sin', 'Month_cos', 'Is_Weekend', 'Sales_Lag_7', 'Rolling_Sales_7_Office Supplies', 'Rolling_Profit_7_Office Supplies', 'Rolling_Sales_7_Furniture', 'Rolling_Profit_7_Furniture', 'Rolling_Sales_7_Technology', 'Rolling_Profit_7_Technology', 'Cumulative_Sales', 'Cumulative_Profit', 'Sales_vs_Category_Avg', 'Category_Sales_Growth', 'Rolling_30_Sales', 'Weekend_Sales']\n",
      "['Order Date', 'Segment', 'Category', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping Cost', 'Order Priority', 'Unit_Price']\n",
      "Total missing values in the combined dataset: 0\n",
      "Shape of the combined dataset: (51244, 28)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_missing_values(df, function_name):\n",
    "    \"\"\"Helper function to check and print missing values after feature creation.\"\"\"\n",
    "    print(f\"Missing values in {function_name}: {df.isna().sum().sum()}\")\n",
    "\n",
    "def create_time_based_features(df):\n",
    "    \"\"\"Create time-based features with cyclic encoding.\"\"\"\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Order Date']):\n",
    "        df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')\n",
    "\n",
    "    df['Order Date'] = df['Order Date'].fillna(pd.Timestamp('2000-01-01'))  \n",
    "\n",
    "    features_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Cyclic encoding for Day_of_Week and Month\n",
    "    features_df['Day_of_Week_sin'] = np.sin(2 * np.pi * df['Order Date'].dt.dayofweek / 7)\n",
    "    features_df['Day_of_Week_cos'] = np.cos(2 * np.pi * df['Order Date'].dt.dayofweek / 7)\n",
    "    features_df['Month_sin'] = np.sin(2 * np.pi * df['Order Date'].dt.month / 12)\n",
    "    features_df['Month_cos'] = np.cos(2 * np.pi * df['Order Date'].dt.month / 12)\n",
    "\n",
    "    # Add Is_Weekend (binary feature)\n",
    "    features_df['Is_Weekend'] = df['Order Date'].dt.dayofweek.apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "    check_missing_values(features_df, \"Time-Based Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_lag_features(df, features_df):\n",
    "    \"\"\"Create lag features and add to the existing features_df.\"\"\"\n",
    "    \n",
    "    features_df['Sales_Lag_7'] = df['Sales'].shift(7)\n",
    "\n",
    "    check_missing_values(features_df, \"Lag Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_rolling_features(df, features_df):\n",
    "    \"\"\"Create rolling window statistics for each category.\"\"\"\n",
    "    categories = df['Category'].unique()\n",
    "\n",
    "    for category in categories:\n",
    "        cat_df = df[df['Category'] == category]\n",
    "\n",
    "        rolling_sales = cat_df['Sales'].rolling(window=7, min_periods=7).mean()\n",
    "        rolling_profit = cat_df['Profit'].rolling(window=7, min_periods=7).mean()\n",
    "\n",
    "        features_df.loc[cat_df.index, f'Rolling_Sales_7_{category}'] = rolling_sales\n",
    "        features_df.loc[cat_df.index, f'Rolling_Profit_7_{category}'] = rolling_profit\n",
    "\n",
    "    for col in features_df.columns:\n",
    "        if 'Rolling_Sales_7' in col:\n",
    "            features_df[col] = features_df[col].ffill()  \n",
    "        elif 'Rolling_Profit_7' in col:\n",
    "            features_df[col] = features_df[col].bfill()  \n",
    "\n",
    "    check_missing_values(features_df, \"Rolling Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_cumulative_features(df, features_df):\n",
    "    \"\"\"Create cumulative features.\"\"\"\n",
    "    features_df['Cumulative_Sales'] = df.groupby('Category')['Sales'].cumsum()\n",
    "    features_df['Cumulative_Profit'] = df.groupby('Category')['Profit'].cumsum()\n",
    "    \n",
    "    check_missing_values(features_df, \"Cumulative Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_category_level_features(df, features_df):\n",
    "    \"\"\"Create category-level features.\"\"\"\n",
    "    category_sales_avg = df.groupby('Category')['Sales'].transform('mean')\n",
    "    \n",
    "    features_df['Sales_vs_Category_Avg'] = df['Sales'] - category_sales_avg\n",
    "    \n",
    "    check_missing_values(features_df, \"Category-Level Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_growth_and_extra_features(df, features_df):\n",
    "    \"\"\"Create growth and extra features.\"\"\"\n",
    "    features_df['Category_Sales_Growth'] = df.groupby('Category')['Sales'].pct_change()\n",
    "    features_df['Rolling_30_Sales'] = df.groupby('Category')['Sales'].rolling(30).mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    check_missing_values(features_df, \"Growth and Extra Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_interaction_features(df, features_df):\n",
    "    \"\"\"Create interaction features between time-based and categorical/statistical features.\"\"\"\n",
    "    features_df['Weekend_Sales'] = features_df['Is_Weekend'] * df['Sales']\n",
    "    \n",
    "    \n",
    "    check_missing_values(features_df, \"Interaction Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def handle_missing_values(df, features_df):\n",
    "    \"\"\"Handle missing values created by lagged features, rolling statistics, and growth/extra features.\"\"\"\n",
    "    # Dynamically identify columns with NaN values\n",
    "    nan_columns = features_df.columns[features_df.isna().any()].tolist()\n",
    "    \n",
    "    # Fill NaN values based on column type or name\n",
    "    for col in nan_columns:\n",
    "        if 'Rolling' in col:  # Handle rolling features\n",
    "            if 'Sales' in col:\n",
    "                features_df[col] = features_df[col].fillna(df['Sales'].mean())\n",
    "            elif 'Profit' in col:\n",
    "                features_df[col] = features_df[col].fillna(df['Profit'].mean())\n",
    "            else:\n",
    "                features_df[col] = features_df[col].fillna(0)  # Default to 0 for other rolling features\n",
    "        elif 'Lag' in col:  # Handle lag features\n",
    "            features_df[col] = features_df[col].fillna(df['Sales'].mean())\n",
    "        elif 'Growth' in col:  # Handle growth features\n",
    "            features_df[col] = features_df[col].fillna(0)  # Default to 0 for growth features\n",
    "        else:  # Handle all other columns\n",
    "            features_df[col] = features_df[col].fillna(0)  # Default to 0 for unknown columns\n",
    "    \n",
    "    # Re-check for missing values after handling\n",
    "    check_missing_values(features_df, \"Missing Value Handling\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "\n",
    "# Call the function to create time-based features\n",
    "features_df = create_time_based_features(cleaned_df)\n",
    "\n",
    "# Call the function to create lag features and add them to the same features_df\n",
    "features_df = create_lag_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to create rolling features and add them to the same features_df\n",
    "features_df = create_rolling_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to create cumulative features and add them to the same features_df\n",
    "features_df = create_cumulative_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to create category-level features and add them to the same features_df\n",
    "features_df = create_category_level_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to create growth and extra features and add them to the same features_df\n",
    "features_df = create_growth_and_extra_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to create interaction features and add them to the same features_df\n",
    "features_df = create_interaction_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to handle missing values\n",
    "features_df = handle_missing_values(cleaned_df, features_df)\n",
    "\n",
    "# Print the first few rows to check the result\n",
    "print(features_df.head(10))\n",
    "print(features_df.columns.tolist())\n",
    "print(cleaned_df.columns.tolist())\n",
    "\n",
    "# Combine the original dataframe with the engineered features\n",
    "df_combined = pd.concat([cleaned_df, features_df], axis=1)\n",
    "\n",
    "# Optionally, reset the index if needed\n",
    "df_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Total missing values in the combined dataset: {df_combined.isna().sum().sum()}\")\n",
    "print(f\"Shape of the combined dataset: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Order Date  Segment    Sales  Quantity  Discount  Profit  Shipping Cost  \\\n",
      "0 2011-01-01        0   66.120       4.0       0.0  29.640           8.17   \n",
      "1 2011-01-01        0  120.366       3.0       0.1  36.036           9.72   \n",
      "2 2011-01-01        2   44.865       3.0       0.5 -26.055           4.82   \n",
      "3 2011-01-01        0   55.242       2.0       0.1  15.342           1.80   \n",
      "4 2011-01-01        0  113.670       5.0       0.1  37.770           4.70   \n",
      "\n",
      "   Unit_Price  Day_of_Week_sin  Day_of_Week_cos  ...  Cumulative_Sales  \\\n",
      "0    7.077500        -0.974928        -0.222521  ...            66.120   \n",
      "1   27.633333        -0.974928        -0.222521  ...           186.486   \n",
      "2   44.066667        -0.974928        -0.222521  ...           231.351   \n",
      "3   21.166667        -0.974928        -0.222521  ...           286.593   \n",
      "4   15.822222        -0.974928        -0.222521  ...           113.670   \n",
      "\n",
      "   Cumulative_Profit  Sales_vs_Category_Avg  Category_Sales_Growth  \\\n",
      "0             29.640             -30.066184               0.000000   \n",
      "1             65.676              24.179816               0.820417   \n",
      "2             39.621             -51.321184              -0.627262   \n",
      "3             54.963             -40.944184               0.231294   \n",
      "4             37.770            -165.296440               0.000000   \n",
      "\n",
      "   Rolling_30_Sales  Weekend_Sales  Category_Furniture  \\\n",
      "0        172.398334         66.120                 0.0   \n",
      "1        172.398334        120.366                 0.0   \n",
      "2        172.398334         44.865                 0.0   \n",
      "3        172.398334         55.242                 0.0   \n",
      "4        172.398334        113.670                 1.0   \n",
      "\n",
      "   Category_Office Supplies  Category_Technology  Order Priority_Code  \n",
      "0                       1.0                  0.0                    3  \n",
      "1                       1.0                  0.0                    2  \n",
      "2                       1.0                  0.0                    3  \n",
      "3                       1.0                  0.0                    2  \n",
      "4                       0.0                  0.0                    2  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "def encode_data(df_combined):\n",
    "    # Initialize the OneHotEncoder\n",
    "    onehot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "    # Initialize the LabelEncoder for the 'Segment' column\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Apply One-Hot Encoding to the 'Category' column if it exists\n",
    "    if 'Category' in df_combined.columns:\n",
    "        category_encoder = OneHotEncoder(sparse_output=False)\n",
    "        encoded_category = category_encoder.fit_transform(df_combined[['Category']])\n",
    "        encoded_category_df = pd.DataFrame(encoded_category, columns=category_encoder.get_feature_names_out(['Category']))\n",
    "        \n",
    "        # Drop the original 'Category' column after encoding\n",
    "        df_combined.drop(columns=['Category'], inplace=True)\n",
    "        \n",
    "        # Concatenate the encoded 'Category' columns back to the dataframe\n",
    "        df_combined = pd.concat([df_combined, encoded_category_df], axis=1)\n",
    "\n",
    "    # Label Encode 'Segment' column\n",
    "    if 'Segment' in df_combined.columns:\n",
    "        df_combined['Segment'] = label_encoder.fit_transform(df_combined['Segment'])\n",
    "\n",
    "    # Apply Priority Mapping to 'Order Priority' column\n",
    "    if 'Order Priority' in df_combined.columns:\n",
    "        priority_mapping = {\"low\": 1, \"medium\": 2, \"high\": 3, \"critical\": 4}\n",
    "        df_combined['Order Priority_Code'] = df_combined['Order Priority'].map(priority_mapping)\n",
    "        df_combined.drop('Order Priority', axis=1, inplace=True)  # Drop original Order Priority column in place\n",
    "\n",
    "    # Return the newly encoded DataFrame\n",
    "    return df_combined\n",
    "\n",
    "# Example usage\n",
    "# Load your dataframe here\n",
    "# df_combined = pd.read_csv('your_data.csv')\n",
    "df_combined = encode_data(df_combined)\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of Null in the data : 0\n",
      "Feature_Names :  ['Order Date', 'Segment', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping Cost', 'Unit_Price', 'Day_of_Week_sin', 'Day_of_Week_cos', 'Month_sin', 'Month_cos', 'Is_Weekend', 'Sales_Lag_7', 'Rolling_Sales_7_Office Supplies', 'Rolling_Profit_7_Office Supplies', 'Rolling_Sales_7_Furniture', 'Rolling_Profit_7_Furniture', 'Rolling_Sales_7_Technology', 'Rolling_Profit_7_Technology', 'Cumulative_Sales', 'Cumulative_Profit', 'Sales_vs_Category_Avg', 'Category_Sales_Growth', 'Rolling_30_Sales', 'Weekend_Sales', 'Category_Furniture', 'Category_Office Supplies', 'Category_Technology', 'Order Priority_Code']\n",
      "Shape of the X_train: (40971, 30, 29)\n",
      "Shape of the X_test: (10243, 30, 29)\n",
      "Shape of the y_train: (40971, 30, 1)\n",
      "Shape of the y_test: (10243, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming df_combined is already defined\n",
    "df_processed = df_combined.copy()\n",
    "\n",
    "# Check for null values\n",
    "Total_null = df_processed.isnull().sum().sum()\n",
    "print(\"The total number of Null in the data :\", Total_null)\n",
    "\n",
    "# Store the 'Order Date' column and feature names\n",
    "order_date_stored = df_processed['Order Date'].copy()\n",
    "feature_name_stored = df_processed.columns.tolist()\n",
    "df_processed = df_processed.drop(columns=['Order Date'])\n",
    "\n",
    "print(\"Feature_Names : \", feature_name_stored)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df_processed), columns=df_processed.columns, index=df_processed.index)\n",
    "\n",
    "# Define the target column\n",
    "target_column = 'Sales'\n",
    "\n",
    "# Create sequences\n",
    "time_steps = 30\n",
    "\n",
    "def create_sequence(data, time_steps=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data.iloc[i:i+time_steps].values)\n",
    "        y.append(data.iloc[i+time_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequence(df_normalized, time_steps)\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2])  # (batch_size, seq_length, num_features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Reshape y_train and y_test to (batch_size, seq_length, output_size)\n",
    "y_train = np.expand_dims(y_train, axis=-1)  # (batch_size, 1)\n",
    "y_train = np.tile(y_train, (1, time_steps))  # (batch_size, seq_length)\n",
    "y_train = np.expand_dims(y_train, axis=-1)  # (batch_size, seq_length, 1)\n",
    "\n",
    "y_test = np.expand_dims(y_test, axis=-1)  # (batch_size, 1)\n",
    "y_test = np.tile(y_test, (1, time_steps))  # (batch_size, seq_length)\n",
    "y_test = np.expand_dims(y_test, axis=-1)  # (batch_size, seq_length, 1)\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(\"Shape of the X_train:\", X_train.shape)\n",
    "print(\"Shape of the X_test:\", X_test.shape)\n",
    "print(\"Shape of the y_train:\", y_train.shape)\n",
    "print(\"Shape of the y_test:\", y_test.shape)\n",
    "\n",
    "# Save the arrays to .npy files\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        return outputs, (h_n, c_n)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(hidden_size + output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.attn_vector = nn.Parameter(torch.randn(hidden_size))\n",
    "    \n",
    "    def forward(self, y, encoder_outputs):\n",
    "        outputs = []\n",
    "        decoder_input = y[:, 0].unsqueeze(1)  \n",
    "        for t in range(y.shape[1]):\n",
    "            attn = torch.matmul(encoder_outputs, self.attn_vector)  \n",
    "            attention_weights = torch.softmax(attn, dim=1).unsqueeze(2)  \n",
    "            context = torch.bmm(attention_weights.transpose(1, 2), encoder_outputs) \n",
    "            \n",
    "            lstm_input = torch.cat((decoder_input, context), dim=2)  \n",
    "            output, _ = self.lstm(lstm_input)\n",
    "            output = self.fc(output) \n",
    "            outputs.append(output)\n",
    "            \n",
    "            decoder_input = output  \n",
    "        \n",
    "        return torch.cat(outputs, dim=1)  \n",
    "\n",
    "class DA_RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(DA_RNN, self).__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_size, num_layers)\n",
    "        self.decoder = Decoder(hidden_size, output_size, num_layers)\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        encoder_outputs, (h_n, c_n) = self.encoder(X)\n",
    "        outputs = self.decoder(y, encoder_outputs)\n",
    "        return outputs\n",
    "\n",
    "def train_and_evaluate(model, X, y, optimizer, criterion, device, epochs=10, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "        train_X, test_X = X[train_idx], X[test_idx]\n",
    "        train_y, test_y = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(train_X, train_y)\n",
    "            loss = criterion(outputs, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_y = model(test_X, test_y)\n",
    "            pred_y = pred_y.cpu().numpy()\n",
    "            true_y = test_y.cpu().numpy()\n",
    "            \n",
    "            \n",
    "            true_y = true_y.reshape(-1, true_y.shape[-1])  \n",
    "            pred_y = pred_y.reshape(-1, pred_y.shape[-1]) \n",
    "            \n",
    "            \n",
    "            mse = mean_squared_error(true_y, pred_y)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(true_y, pred_y)\n",
    "            r2 = r2_score(true_y, pred_y)\n",
    "            mape = np.mean(np.abs((true_y - pred_y) / (true_y + 1e-8))) * 100  \n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f} | MSE: {mse:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | R^2: {r2:.4f} | MAPE: {mape:.2f}%\")\n",
    "\n",
    "\n",
    "X = np.load(\"/Users/mohammednihal/Desktop/XAI/model/X_train.npy\")\n",
    "y = np.load(\"/Users/mohammednihal/Desktop/XAI/model/y_train.npy\")\n",
    "\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")  \n",
    "print(f\"Shape of y: {y.shape}\")  \n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "input_size = X.shape[2]\n",
    "hidden_size = 16\n",
    "output_size = y.shape[2]  \n",
    "batch_size = 50\n",
    "seq_length = X.shape[1]\n",
    "n_splits = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DA_RNN(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "X, y = X.to(device), y.to(device)\n",
    "train_and_evaluate(model, X, y, optimizer, criterion, device, epochs=10, n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
