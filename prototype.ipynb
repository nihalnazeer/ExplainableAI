{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Row ID         Order ID  Order Date  ...    Profit Shipping Cost Order Priority\n",
      "0   32298   CA-2012-124891  31-07-2012  ...  762.1845        933.57       Critical\n",
      "1   26341    IN-2013-77878  05-02-2013  ... -288.7650        923.63       Critical\n",
      "2   25330    IN-2013-71249  17-10-2013  ...  919.9710        915.49         Medium\n",
      "3   13524  ES-2013-1579342  28-01-2013  ...  -96.5400        910.16         Medium\n",
      "4   47221     SG-2013-4320  05-11-2013  ...  311.5200        903.04       Critical\n",
      "\n",
      "[5 rows x 24 columns]\n",
      "DataFrame shape : (51290, 24)\n",
      "\n",
      "DataFrame  head:\n",
      "   Row ID         Order ID  Order Date  ...    Profit Shipping Cost Order Priority\n",
      "0   32298   CA-2012-124891  31-07-2012  ...  762.1845        933.57       Critical\n",
      "1   26341    IN-2013-77878  05-02-2013  ... -288.7650        923.63       Critical\n",
      "2   25330    IN-2013-71249  17-10-2013  ...  919.9710        915.49         Medium\n",
      "3   13524  ES-2013-1579342  28-01-2013  ...  -96.5400        910.16         Medium\n",
      "4   47221     SG-2013-4320  05-11-2013  ...  311.5200        903.04       Critical\n",
      "\n",
      "[5 rows x 24 columns]\n",
      "\n",
      "DataFrame columns: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'City', 'State', 'Country', 'Postal Code', 'Market', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping Cost', 'Order Priority']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/mohammednihal/Desktop/XAI/data/Global Superstore.csv',encoding='ISO-8859-1')\n",
    "df = df.drop([col  for col in df.columns if 'Unnamed' in col], axis=1)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"DataFrame shape :\" , df.shape)\n",
    "print(\"\\nDataFrame  head:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataFrame columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Data Overview:\n",
      "Initial shape: (51290, 24)\n",
      "Columns: ['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name', 'Segment', 'City', 'State', 'Country', 'Postal Code', 'Market', 'Region', 'Product ID', 'Category', 'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping Cost', 'Order Priority']\n",
      "\n",
      "Missing Values Overview:\n",
      "Row ID                0\n",
      "Order ID              0\n",
      "Order Date            0\n",
      "Ship Date             0\n",
      "Ship Mode             0\n",
      "Customer ID           0\n",
      "Customer Name         0\n",
      "Segment               0\n",
      "City                  0\n",
      "State                 0\n",
      "Country               0\n",
      "Postal Code       41296\n",
      "Market                0\n",
      "Region                0\n",
      "Product ID            0\n",
      "Category              0\n",
      "Sub-Category          0\n",
      "Product Name          0\n",
      "Sales                 0\n",
      "Quantity              0\n",
      "Discount              0\n",
      "Profit                0\n",
      "Shipping Cost         0\n",
      "Order Priority        0\n",
      "dtype: int64\n",
      "\n",
      "Cleaning Data...\n",
      "   Row ID         Order ID Order Date  ... Shipping Cost Order Priority Unit_Price\n",
      "0   48883     HU-2011-1220 2011-01-01  ...          8.17           high   7.077500\n",
      "1   22253    IN-2011-47883 2011-01-01  ...          9.72         medium  27.633333\n",
      "2   11731  IT-2011-3647632 2011-01-01  ...          4.82           high  44.066667\n",
      "3   22254    IN-2011-47883 2011-01-01  ...          1.80         medium  21.166667\n",
      "4   22255    IN-2011-47883 2011-01-01  ...          4.70         medium  15.822222\n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Data cleaned. Remaining rows: 51244\n",
      "Columns Dropped Successfully\n",
      "Remaining columns: ['Order Date', 'Segment', 'Category', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping Cost', 'Order Priority', 'Unit_Price']\n",
      "\n",
      "Data Cleaning Pipeline Executed Successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def data_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Convert datetime columns\n",
    "    datetime_columns = ['Order Date', 'Ship Date']\n",
    "    for col in datetime_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], dayfirst=True, errors='coerce')\n",
    "\n",
    "    # Remove exact duplicates\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "    # Remove potential duplicates based on Customer ID, Sales, and Profit\n",
    "    df_clean = df_clean.drop_duplicates(subset=['Customer ID', 'Sales', 'Profit'], keep='first')\n",
    "\n",
    "    # Fill missing numerical values with median\n",
    "    numeric_columns = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in numeric_columns:\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "\n",
    "    # Fill missing categorical values with mode\n",
    "    categorical_columns = df_clean.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        if not df_clean[col].mode().empty:\n",
    "            df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0])\n",
    "\n",
    "    # Handle outliers using IQR\n",
    "    def handle_outliers(series):\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return series.clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "    numeric_cols_for_outliers = ['Sales', 'Profit', 'Quantity', 'Discount', 'Shipping Cost']\n",
    "    for col in numeric_cols_for_outliers:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = handle_outliers(df_clean[col])\n",
    "\n",
    "    # Clip specific ranges\n",
    "    df_clean['Profit'] = df_clean['Profit'].clip(lower=-1500, upper=10000)\n",
    "    df_clean['Quantity'] = df_clean['Quantity'].clip(lower=1)\n",
    "    df_clean['Discount'] = df_clean['Discount'].clip(0, 1)\n",
    "    df_clean['Shipping Cost'] = df_clean['Shipping Cost'].clip(lower=0, upper=1000)\n",
    "\n",
    "    # Calculate Unit Price safely\n",
    "    discount_safe = (1 - df_clean['Discount']).replace(0, np.nan)\n",
    "    df_clean['Unit_Price'] = (df_clean['Sales'] - df_clean['Shipping Cost'] - df_clean['Profit']) / (df_clean['Quantity'] * discount_safe)\n",
    "\n",
    "    # Standardize categorical variables\n",
    "    standardize_cols = ['Customer Name', 'City', 'State', 'Product Name', 'Country', 'Order Priority', 'Region', 'Market']\n",
    "    for col in standardize_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].str.lower()\n",
    "\n",
    "    # Remove any remaining NaNs\n",
    "    df_clean = df_clean.dropna()\n",
    "\n",
    "    # Sort by Order Date\n",
    "    if 'Order Date' in df_clean.columns:\n",
    "        df_clean = df_clean.sort_values(by='Order Date', ascending=True)\n",
    "\n",
    "    df_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(df_clean.head())\n",
    "\n",
    "    # Optional: Save to CSV\n",
    "    df_clean.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def Initial_col_drop(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    columns_to_drop = [\n",
    "        'Row ID', 'Order ID', 'Ship Date', 'Ship Mode', 'Customer ID', 'Customer Name',\n",
    "        'City', 'State', 'Country', 'Postal Code', 'Market', 'Region', 'Product ID',\n",
    "        'Product Name', 'Year-Month', 'Sub-Category'\n",
    "    ]\n",
    "    return df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "\n",
    "\n",
    "def main(df: pd.DataFrame):\n",
    "    \n",
    "    try:\n",
    "        # Initial data overview\n",
    "        print(\"\\nInitial Data Overview:\")\n",
    "        print(f\"Initial shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        print(\"\\nMissing Values Overview:\")\n",
    "        print(df.isnull().sum())\n",
    "\n",
    "        # Cleaning data\n",
    "        print(\"\\nCleaning Data...\")\n",
    "        cleaned_df = data_cleaning(df)\n",
    "        print(f\"Data cleaned. Remaining rows: {len(cleaned_df)}\")\n",
    "\n",
    "        # Dropping unnecessary columns\n",
    "        cleaned_df = Initial_col_drop(cleaned_df)\n",
    "        print(\"Columns Dropped Successfully\")\n",
    "        print(f\"Remaining columns: {cleaned_df.columns.tolist()}\")\n",
    "\n",
    "        return cleaned_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Data Cleaning Pipeline: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    try:\n",
    "        df = df  \n",
    "        cleaned_df = main(df)\n",
    "\n",
    "        if cleaned_df is not None:\n",
    "            print(\"\\nData Cleaning Pipeline Executed Successfully!\")\n",
    "        else:\n",
    "            print(\"\\nData Cleaning Pipeline Failed.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Data file not found. Please ensure 'your_data_file.csv' exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Office Supplies\n",
      "1    Office Supplies\n",
      "2    Office Supplies\n",
      "3    Office Supplies\n",
      "4          Furniture\n",
      "5    Office Supplies\n",
      "6         Technology\n",
      "7         Technology\n",
      "8          Furniture\n",
      "9    Office Supplies\n",
      "Name: Category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_df['Category'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2011-01-01\n",
      "1   2011-01-01\n",
      "2   2011-01-01\n",
      "3   2011-01-01\n",
      "4   2011-01-01\n",
      "Name: Order Date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "column_name = 'Order Date'\n",
    "print(cleaned_df[column_name].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Time-Based Features: 0\n",
      "Missing values in Lag Features: 7\n",
      "Missing values in Rolling Features: 93\n",
      "Missing values in Cumulative Features: 93\n",
      "Missing values in Category-Level Features: 93\n",
      "Missing values in Growth and Extra Features: 183\n",
      "Missing values in Interaction Features: 183\n",
      "Missing values in Missing Value Handling: 0\n",
      "   Day_of_Week_sin  Day_of_Week_cos  ...  Rolling_30_Sales  Weekend_Sales\n",
      "0        -0.974928        -0.222521  ...        172.398334         66.120\n",
      "1        -0.974928        -0.222521  ...        172.398334        120.366\n",
      "2        -0.974928        -0.222521  ...        172.398334         44.865\n",
      "3        -0.974928        -0.222521  ...        172.398334         55.242\n",
      "4        -0.974928        -0.222521  ...        172.398334        113.670\n",
      "5        -0.974928        -0.222521  ...        172.398334        408.300\n",
      "6        -0.781831         0.623490  ...        172.398334        314.220\n",
      "7         0.000000         1.000000  ...        172.398334          0.000\n",
      "8         0.000000         1.000000  ...        172.398334          0.000\n",
      "9         0.000000         1.000000  ...        172.398334          0.000\n",
      "\n",
      "[10 rows x 18 columns]\n",
      "['Day_of_Week_sin', 'Day_of_Week_cos', 'Month_sin', 'Month_cos', 'Is_Weekend', 'Sales_Lag_7', 'Rolling_Sales_7_Office Supplies', 'Rolling_Profit_7_Office Supplies', 'Rolling_Sales_7_Furniture', 'Rolling_Profit_7_Furniture', 'Rolling_Sales_7_Technology', 'Rolling_Profit_7_Technology', 'Cumulative_Sales', 'Cumulative_Profit', 'Sales_vs_Category_Avg', 'Category_Sales_Growth', 'Rolling_30_Sales', 'Weekend_Sales']\n",
      "['Order Date', 'Segment', 'Category', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping Cost', 'Order Priority', 'Unit_Price']\n",
      "Total missing values in the combined dataset: 0\n",
      "Shape of the combined dataset: (51244, 28)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def check_missing_values(df, function_name):\n",
    "    \"\"\"Helper function to check and print missing values after feature creation.\"\"\"\n",
    "    print(f\"Missing values in {function_name}: {df.isna().sum().sum()}\")\n",
    "\n",
    "def create_time_based_features(df):\n",
    "    \"\"\"Create time-based features with cyclic encoding.\"\"\"\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['Order Date']):\n",
    "        df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')\n",
    "\n",
    "    df['Order Date'] = df['Order Date'].fillna(pd.Timestamp('2000-01-01'))  \n",
    "\n",
    "    features_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Cyclic encoding for Day_of_Week and Month\n",
    "    features_df['Day_of_Week_sin'] = np.sin(2 * np.pi * df['Order Date'].dt.dayofweek / 7)\n",
    "    features_df['Day_of_Week_cos'] = np.cos(2 * np.pi * df['Order Date'].dt.dayofweek / 7)\n",
    "    features_df['Month_sin'] = np.sin(2 * np.pi * df['Order Date'].dt.month / 12)\n",
    "    features_df['Month_cos'] = np.cos(2 * np.pi * df['Order Date'].dt.month / 12)\n",
    "\n",
    "    # Add Is_Weekend (binary feature)\n",
    "    features_df['Is_Weekend'] = df['Order Date'].dt.dayofweek.apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "    check_missing_values(features_df, \"Time-Based Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_lag_features(df, features_df):\n",
    "    \"\"\"Create lag features and add to the existing features_df.\"\"\"\n",
    "    \n",
    "    features_df['Sales_Lag_7'] = df['Sales'].shift(7)\n",
    "\n",
    "    check_missing_values(features_df, \"Lag Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_rolling_features(df, features_df):\n",
    "    \"\"\"Create rolling window statistics for each category.\"\"\"\n",
    "    categories = df['Category'].unique()\n",
    "\n",
    "    for category in categories:\n",
    "        cat_df = df[df['Category'] == category]\n",
    "\n",
    "        rolling_sales = cat_df['Sales'].rolling(window=7, min_periods=7).mean()\n",
    "        rolling_profit = cat_df['Profit'].rolling(window=7, min_periods=7).mean()\n",
    "\n",
    "        features_df.loc[cat_df.index, f'Rolling_Sales_7_{category}'] = rolling_sales\n",
    "        features_df.loc[cat_df.index, f'Rolling_Profit_7_{category}'] = rolling_profit\n",
    "\n",
    "    for col in features_df.columns:\n",
    "        if 'Rolling_Sales_7' in col:\n",
    "            features_df[col] = features_df[col].ffill()  \n",
    "        elif 'Rolling_Profit_7' in col:\n",
    "            features_df[col] = features_df[col].bfill()  \n",
    "\n",
    "    check_missing_values(features_df, \"Rolling Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_cumulative_features(df, features_df):\n",
    "    \"\"\"Create cumulative features.\"\"\"\n",
    "    features_df['Cumulative_Sales'] = df.groupby('Category')['Sales'].cumsum()\n",
    "    features_df['Cumulative_Profit'] = df.groupby('Category')['Profit'].cumsum()\n",
    "    \n",
    "    check_missing_values(features_df, \"Cumulative Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_category_level_features(df, features_df):\n",
    "    \"\"\"Create category-level features.\"\"\"\n",
    "    category_sales_avg = df.groupby('Category')['Sales'].transform('mean')\n",
    "    \n",
    "    features_df['Sales_vs_Category_Avg'] = df['Sales'] - category_sales_avg\n",
    "    \n",
    "    check_missing_values(features_df, \"Category-Level Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_growth_and_extra_features(df, features_df):\n",
    "    \"\"\"Create growth and extra features.\"\"\"\n",
    "    features_df['Category_Sales_Growth'] = df.groupby('Category')['Sales'].pct_change()\n",
    "    features_df['Rolling_30_Sales'] = df.groupby('Category')['Sales'].rolling(30).mean().reset_index(level=0, drop=True)\n",
    "    \n",
    "    check_missing_values(features_df, \"Growth and Extra Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def create_interaction_features(df, features_df):\n",
    "    \"\"\"Create interaction features between time-based and categorical/statistical features.\"\"\"\n",
    "    features_df['Weekend_Sales'] = features_df['Is_Weekend'] * df['Sales']\n",
    "    \n",
    "    \n",
    "    check_missing_values(features_df, \"Interaction Features\")\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def handle_missing_values(df, features_df):\n",
    "    \"\"\"Handle missing values created by lagged features, rolling statistics, and growth/extra features.\"\"\"\n",
    "    # Dynamically identify columns with NaN values\n",
    "    nan_columns = features_df.columns[features_df.isna().any()].tolist()\n",
    "    \n",
    "    # Fill NaN values based on column type or name\n",
    "    for col in nan_columns:\n",
    "        if 'Rolling' in col:  # Handle rolling features\n",
    "            if 'Sales' in col:\n",
    "                features_df[col] = features_df[col].fillna(df['Sales'].mean())\n",
    "            elif 'Profit' in col:\n",
    "                features_df[col] = features_df[col].fillna(df['Profit'].mean())\n",
    "            else:\n",
    "                features_df[col] = features_df[col].fillna(0)  # Default to 0 for other rolling features\n",
    "        elif 'Lag' in col:  # Handle lag features\n",
    "            features_df[col] = features_df[col].fillna(df['Sales'].mean())\n",
    "        elif 'Growth' in col:  # Handle growth features\n",
    "            features_df[col] = features_df[col].fillna(0)  # Default to 0 for growth features\n",
    "        else:  # Handle all other columns\n",
    "            features_df[col] = features_df[col].fillna(0)  # Default to 0 for unknown columns\n",
    "    \n",
    "    # Re-check for missing values after handling\n",
    "    check_missing_values(features_df, \"Missing Value Handling\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "\n",
    "# Call the function to create time-based features\n",
    "features_df = create_time_based_features(cleaned_df)\n",
    "\n",
    "# Call the function to create lag features and add them to the same features_df\n",
    "features_df = create_lag_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to create rolling features and add them to the same features_df\n",
    "features_df = create_rolling_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to create cumulative features and add them to the same features_df\n",
    "features_df = create_cumulative_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to create category-level features and add them to the same features_df\n",
    "features_df = create_category_level_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to create growth and extra features and add them to the same features_df\n",
    "features_df = create_growth_and_extra_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to create interaction features and add them to the same features_df\n",
    "features_df = create_interaction_features(cleaned_df, features_df)\n",
    "\n",
    "# Call the function to handle missing values\n",
    "features_df = handle_missing_values(cleaned_df, features_df)\n",
    "\n",
    "# Print the first few rows to check the result\n",
    "print(features_df.head(10))\n",
    "print(features_df.columns.tolist())\n",
    "print(cleaned_df.columns.tolist())\n",
    "\n",
    "# Combine the original dataframe with the engineered features\n",
    "df_combined = pd.concat([cleaned_df, features_df], axis=1)\n",
    "\n",
    "# Optionally, reset the index if needed\n",
    "df_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Total missing values in the combined dataset: {df_combined.isna().sum().sum()}\")\n",
    "print(f\"Shape of the combined dataset: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Order Date  Segment  ...  Category_Technology  Order Priority_Code\n",
      "0 2011-01-01        0  ...                  0.0                    3\n",
      "1 2011-01-01        0  ...                  0.0                    2\n",
      "2 2011-01-01        2  ...                  0.0                    3\n",
      "3 2011-01-01        0  ...                  0.0                    2\n",
      "4 2011-01-01        0  ...                  0.0                    2\n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "def encode_data(df_combined):\n",
    "    # Initialize the OneHotEncoder\n",
    "    onehot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "    # Initialize the LabelEncoder for the 'Segment' column\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Apply One-Hot Encoding to the 'Category' column if it exists\n",
    "    if 'Category' in df_combined.columns:\n",
    "        category_encoder = OneHotEncoder(sparse_output=False)\n",
    "        encoded_category = category_encoder.fit_transform(df_combined[['Category']])\n",
    "        encoded_category_df = pd.DataFrame(encoded_category, columns=category_encoder.get_feature_names_out(['Category']))\n",
    "        \n",
    "        # Drop the original 'Category' column after encoding\n",
    "        df_combined.drop(columns=['Category'], inplace=True)\n",
    "        \n",
    "        # Concatenate the encoded 'Category' columns back to the dataframe\n",
    "        df_combined = pd.concat([df_combined, encoded_category_df], axis=1)\n",
    "\n",
    "    # Label Encode 'Segment' column\n",
    "    if 'Segment' in df_combined.columns:\n",
    "        df_combined['Segment'] = label_encoder.fit_transform(df_combined['Segment'])\n",
    "\n",
    "    # Apply Priority Mapping to 'Order Priority' column\n",
    "    if 'Order Priority' in df_combined.columns:\n",
    "        priority_mapping = {\"low\": 1, \"medium\": 2, \"high\": 3, \"critical\": 4}\n",
    "        df_combined['Order Priority_Code'] = df_combined['Order Priority'].map(priority_mapping)\n",
    "        df_combined.drop('Order Priority', axis=1, inplace=True)  # Drop original Order Priority column in place\n",
    "\n",
    "    # Return the newly encoded DataFrame\n",
    "    return df_combined\n",
    "\n",
    "# Example usage\n",
    "# Load your dataframe here\n",
    "# df_combined = pd.read_csv('your_data.csv')\n",
    "df_combined = encode_data(df_combined)\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of Null in the data : 0\n",
      "Feature_Names :  ['Order Date', 'Segment', 'Sales', 'Quantity', 'Discount', 'Profit', 'Shipping Cost', 'Unit_Price', 'Day_of_Week_sin', 'Day_of_Week_cos', 'Month_sin', 'Month_cos', 'Is_Weekend', 'Sales_Lag_7', 'Rolling_Sales_7_Office Supplies', 'Rolling_Profit_7_Office Supplies', 'Rolling_Sales_7_Furniture', 'Rolling_Profit_7_Furniture', 'Rolling_Sales_7_Technology', 'Rolling_Profit_7_Technology', 'Cumulative_Sales', 'Cumulative_Profit', 'Sales_vs_Category_Avg', 'Category_Sales_Growth', 'Rolling_30_Sales', 'Weekend_Sales', 'Category_Furniture', 'Category_Office Supplies', 'Category_Technology', 'Order Priority_Code']\n",
      "[[[0.         0.11301313 0.35294118 ... 1.         0.         0.66666667]\n",
      "  [0.         0.20635789 0.23529412 ... 1.         0.         0.33333333]\n",
      "  [1.         0.07643822 0.23529412 ... 1.         0.         0.66666667]\n",
      "  ...\n",
      "  [0.5        0.02850624 0.11764706 ... 1.         0.         0.33333333]\n",
      "  [0.         0.02753916 0.11764706 ... 1.         0.         0.66666667]\n",
      "  [1.         0.06562319 0.23529412 ... 1.         0.         0.33333333]]\n",
      "\n",
      " [[0.         0.20635789 0.23529412 ... 1.         0.         0.33333333]\n",
      "  [1.         0.07643822 0.23529412 ... 1.         0.         0.66666667]\n",
      "  [0.         0.09429462 0.11764706 ... 1.         0.         0.33333333]\n",
      "  ...\n",
      "  [0.         0.02753916 0.11764706 ... 1.         0.         0.66666667]\n",
      "  [1.         0.06562319 0.23529412 ... 1.         0.         0.33333333]\n",
      "  [0.5        0.04507723 0.11764706 ... 1.         0.         0.33333333]]\n",
      "\n",
      " [[1.         0.07643822 0.23529412 ... 1.         0.         0.66666667]\n",
      "  [0.         0.09429462 0.11764706 ... 1.         0.         0.33333333]\n",
      "  [0.         0.19483563 0.47058824 ... 0.         0.         0.33333333]\n",
      "  ...\n",
      "  [1.         0.06562319 0.23529412 ... 1.         0.         0.33333333]\n",
      "  [0.5        0.04507723 0.11764706 ... 1.         0.         0.33333333]\n",
      "  [1.         0.06434983 0.11764706 ... 0.         1.         0.33333333]]\n",
      "\n",
      " [[0.         0.09429462 0.11764706 ... 1.         0.         0.33333333]\n",
      "  [0.         0.19483563 0.47058824 ... 0.         0.         0.33333333]\n",
      "  [0.         0.70182539 0.11764706 ... 1.         0.         0.33333333]\n",
      "  ...\n",
      "  [0.5        0.04507723 0.11764706 ... 1.         0.         0.33333333]\n",
      "  [1.         0.06434983 0.11764706 ... 0.         1.         0.33333333]\n",
      "  [1.         0.12709589 0.58823529 ... 1.         0.         0.33333333]]\n",
      "\n",
      " [[0.         0.19483563 0.47058824 ... 0.         0.         0.33333333]\n",
      "  [0.         0.70182539 0.11764706 ... 1.         0.         0.33333333]\n",
      "  [0.         0.53993557 0.         ... 0.         1.         0.33333333]\n",
      "  ...\n",
      "  [1.         0.06434983 0.11764706 ... 0.         1.         0.33333333]\n",
      "  [1.         0.12709589 0.58823529 ... 1.         0.         0.33333333]\n",
      "  [0.         0.04631618 0.23529412 ... 1.         0.         0.33333333]]]\n",
      "Shape of the X_train:  (40971, 30, 29)\n",
      "Shape of the X_test: (10243, 30, 29)\n",
      "Shape of the y_train:  (40971,)\n",
      "Shape of the y_test (10243,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df_processed = df_combined.copy()\n",
    "\n",
    "Total_null = df_processed.isnull().sum().sum()\n",
    "print(\"The total number of Null in the data :\" , Total_null)\n",
    "\n",
    "order_date_stored = df_processed['Order Date'].copy()\n",
    "feature_name_stored = df_processed.columns.tolist()\n",
    "df_processed = df_processed.drop(columns=['Order Date'])\n",
    "\n",
    "print(\"Feature_Names : \", feature_name_stored)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df_processed), columns = df_processed.columns, index = df_processed.index)\n",
    "#print(df_normalized.head())\n",
    "\n",
    "\n",
    "time_steps = 30\n",
    "\n",
    "def create_sequence(data, time_steps = 30):\n",
    "    X, y = [],[]\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data.iloc[i:i+time_steps].values)\n",
    "        y.append(data.iloc[i+time_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "target_column = 'Sales'\n",
    "\n",
    "X,y = create_sequence(df_normalized, time_steps)\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "print(X_train[:5])\n",
    "print(\"Shape of the X_train: \",X_train.shape)\n",
    "print(\"Shape of the X_test:\",X_test.shape)\n",
    "print(\"Shape of the y_train: \",y_train.shape)\n",
    "print(\"Shape of the y_test\",y_test.shape)\n",
    "\n",
    "np.save('X_train.npy',X_train)\n",
    "np.save('X_test.npy',X_test)\n",
    "np.save('y_train.npy',y_train)\n",
    "np.save('y_test.npy',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
